{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25ff8f3",
   "metadata": {},
   "source": [
    "# Introduction to NLP\n",
    "\n",
    "\n",
    " A notebook for the concepts presented - [A Gentle Introduction to Natural Language Processing](https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955222e",
   "metadata": {},
   "source": [
    "## Basic Lib Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6823454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!conda install --yes nltk\n",
    "!conda install --yes numpy\n",
    "!conda install --yes scikit-learn\n",
    "!conda install --yes pandas\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8697d5c4",
   "metadata": {},
   "source": [
    "## Running Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9878ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p> Instagram (from Facebook) allows you to create and share your photos, stories, and videos with the friends and followers you care about. Connect with friends, share what you're up to, or see what's new from others all over the world. Explore our community where you can feel free to be yourself and share everything from your daily moments to life's highlights.</p>\n"
     ]
    }
   ],
   "source": [
    "# sample description from Google App store\n",
    "\n",
    "description = \"<p> Instagram (from Facebook) allows you to create and share your photos, stories, and videos with the friends and followers you care about. Connect with friends, share what you're up to, or see what's new from others all over the world. Explore our community where you can feel free to be yourself and share everything from your daily moments to life's highlights.</p>\"\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a708e80",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "\n",
    "Example removing html characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2249a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instagram  allows you to create and share your photos, stories, and videos with the friends and followers you care about. Connect with friends, share what you're up to, or see what's new from others all over the world. Explore our community where you can feel free to be yourself and share everything from your daily moments to life's highlights.\n"
     ]
    }
   ],
   "source": [
    "cleaned_description = re.sub(re.compile('<.*?>'), \"\", description)\n",
    "cleaned_description = re.sub(re.compile('\\(.*?\\)'), \"\", cleaned_description)\n",
    "cleaned_description = cleaned_description.strip()\n",
    "print(cleaned_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377080d",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "\n",
    "Example converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f27d755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instagram  allows you to create and share your photos, stories, and videos with the friends and followers you care about. connect with friends, share what you're up to, or see what's new from others all over the world. explore our community where you can feel free to be yourself and share everything from your daily moments to life's highlights.\n"
     ]
    }
   ],
   "source": [
    "cleaned_description = cleaned_description.lower()\n",
    "print(cleaned_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccff290",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83fed2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instagram  allows you to create and share your photos, stories, and videos with the friends and followers you care about. connect with friends, share what you're up to, or see what's new from others all over the world. explore our community where you can feel free to be yourself and share everything from your daily moments to life's highlights.\n",
      "['instagram', 'allows', 'you', 'to', 'create', 'and', 'share', 'your', 'photos', ',', 'stories', ',', 'and', 'videos', 'with', 'the', 'friends', 'and', 'followers', 'you', 'care', 'about', '.', 'connect', 'with', 'friends', ',', 'share', 'what', 'you', \"'re\", 'up', 'to', ',', 'or', 'see', 'what', \"'s\", 'new', 'from', 'others', 'all', 'over', 'the', 'world', '.', 'explore', 'our', 'community', 'where', 'you', 'can', 'feel', 'free', 'to', 'be', 'yourself', 'and', 'share', 'everything', 'from', 'your', 'daily', 'moments', 'to', 'life', \"'s\", 'highlights', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "tokens = nltk.word_tokenize(cleaned_description)\n",
    "\n",
    "print(cleaned_description)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2939f",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3990c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instagram', 'allows', 'create', 'share', 'photos', ',', 'stories', ',', 'videos', 'friends', 'followers', 'care', '.', 'connect', 'friends', ',', 'share', \"'re\", ',', 'see', \"'s\", 'new', 'others', 'world', '.', 'explore', 'community', 'feel', 'free', 'share', 'everything', 'daily', 'moments', 'life', \"'s\", 'highlights', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "filtered_description = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60fd7f",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a123f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instagram', 'allow', 'creat', 'share', 'photo', ',', 'stori', ',', 'video', 'friend', 'follow', 'care', '.', 'connect', 'friend', ',', 'share', \"'re\", ',', 'see', \"'s\", 'new', 'other', 'world', '.', 'explor', 'commun', 'feel', 'free', 'share', 'everyth', 'daili', 'moment', 'life', \"'s\", 'highlight', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_description = [stemmer.stem(word) for word in filtered_description]\n",
    "print(stemmed_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d163572",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4cd3039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instagram', 'allows', 'create', 'share', 'photo', ',', 'story', ',', 'video', 'friend', 'follower', 'care', '.', 'connect', 'friend', ',', 'share', \"'re\", ',', 'see', \"'s\", 'new', 'others', 'world', '.', 'explore', 'community', 'feel', 'free', 'share', 'everything', 'daily', 'moment', 'life', \"'s\", 'highlight', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_description = [lemmatizer.lemmatize(word) for word in filtered_description]\n",
    "print(lemma_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f064f",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796a56d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('instagram', 'allows', 'you'), ('allows', 'you', 'to'), ('you', 'to', 'create'), ('to', 'create', 'and'), ('create', 'and', 'share'), ('and', 'share', 'your'), ('share', 'your', 'photos,'), ('your', 'photos,', 'stories,'), ('photos,', 'stories,', 'and'), ('stories,', 'and', 'videos'), ('and', 'videos', 'with'), ('videos', 'with', 'the'), ('with', 'the', 'friends'), ('the', 'friends', 'and'), ('friends', 'and', 'followers'), ('and', 'followers', 'you'), ('followers', 'you', 'care'), ('you', 'care', 'about.'), ('care', 'about.', 'connect'), ('about.', 'connect', 'with'), ('connect', 'with', 'friends,'), ('with', 'friends,', 'share'), ('friends,', 'share', 'what'), ('share', 'what', \"you're\"), ('what', \"you're\", 'up'), (\"you're\", 'up', 'to,'), ('up', 'to,', 'or'), ('to,', 'or', 'see'), ('or', 'see', \"what's\"), ('see', \"what's\", 'new'), (\"what's\", 'new', 'from'), ('new', 'from', 'others'), ('from', 'others', 'all'), ('others', 'all', 'over'), ('all', 'over', 'the'), ('over', 'the', 'world.'), ('the', 'world.', 'explore'), ('world.', 'explore', 'our'), ('explore', 'our', 'community'), ('our', 'community', 'where'), ('community', 'where', 'you'), ('where', 'you', 'can'), ('you', 'can', 'feel'), ('can', 'feel', 'free'), ('feel', 'free', 'to'), ('free', 'to', 'be'), ('to', 'be', 'yourself'), ('be', 'yourself', 'and'), ('yourself', 'and', 'share'), ('and', 'share', 'everything'), ('share', 'everything', 'from'), ('everything', 'from', 'your'), ('from', 'your', 'daily'), ('your', 'daily', 'moments'), ('daily', 'moments', 'to'), ('moments', 'to', \"life's\"), ('to', \"life's\", 'highlights.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "n = 3\n",
    "trigrams = [grams for grams in ngrams(cleaned_description.split(), n)]\n",
    "print(trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0a8b4",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1e16050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instagram', 'allows', 'you', 'to', 'create', 'and', 'share', 'your', 'photos', ',', 'stories', ',', 'and', 'videos', 'with', 'the', 'friends', 'and', 'followers', 'you', 'care', 'about', '.', 'connect', 'with', 'friends', ',', 'share', 'what', 'you', \"'re\", 'up', 'to', ',', 'or', 'see', 'what', \"'s\", 'new', 'from', 'others', 'all', 'over', 'the', 'world', '.', 'explore', 'our', 'community', 'where', 'you', 'can', 'feel', 'free', 'to', 'be', 'yourself', 'and', 'share', 'everything', 'from', 'your', 'daily', 'moments', 'to', 'life', \"'s\", 'highlights', '.']\n",
      "\n",
      "\n",
      "[\"'re\", \"'s\", ',', '.', 'about', 'all', 'allows', 'and', 'be', 'can', 'care', 'community', 'connect', 'create', 'daily', 'everything', 'explore', 'feel', 'followers', 'free', 'friends', 'from', 'highlights', 'instagram', 'life', 'moments', 'new', 'or', 'others', 'our', 'over', 'photos', 'see', 'share', 'stories', 'the', 'to', 'up', 'videos', 'what', 'where', 'with', 'world', 'you', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "\n",
    "bag_words = np.unique(np.array(tokens)).tolist()\n",
    "print('\\n')\n",
    "print(bag_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38563c",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b6e07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rehman is a software engineering researcher', 'Akond is a researcher', 'Akond is also a professor']\n",
      "\n",
      "\n",
      "               TF-IDF\n",
      "akond        0.000000\n",
      "also         0.000000\n",
      "professor    0.000000\n",
      "is           0.298032\n",
      "researcher   0.383770\n",
      "engineering  0.504611\n",
      "rehman       0.504611\n",
      "software     0.504611\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Document1= \"Rehman is a software engineering researcher\"\n",
    "Document2= \"Akond is a researcher\"\n",
    "Document3= \"Akond is also a professor\"\n",
    "Doc = [Document1 , Document2 , Document3]\n",
    "print(Doc)\n",
    "print(\"\\n\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfIdf = vectorizer.fit_transform(Doc)\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=True)\n",
    "print (df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c64a5",
   "metadata": {},
   "source": [
    "## Stanford Typed Dependencies\n",
    "\n",
    "https://corenlp.run/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b8ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
